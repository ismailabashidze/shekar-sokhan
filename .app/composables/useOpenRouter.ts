interface ChatMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

// Advanced AI Settings Configuration System
interface AIResponseConfig {
  max_tokens: number
  temperature: number
  response_format?: { type: string }
  system_prompt_additions: string
  post_processing: {
    enable_emoji_injection: boolean
    emoji_density: number
    enable_formatting: boolean
    format_type: string
  }
}

// Calculate dynamic max_tokens based on length preference
function calculateMaxTokens(lengthPref: string, isPremium: boolean): number {
  const baseTokens = {
    short: 0,
    medium: 0,
    long: 0,
  }
  return baseTokens[lengthPref] || baseTokens.medium
}

// Map creativity to temperature (0-2 scale)
function mapCreativityToTemperature(creativity: string): number {
  const mapping = {
    0: 0.2, // Very focused and deterministic
    1: 0.7, // Balanced
    2: 1.2, // Highly creative
  }
  return mapping[creativity] || 0.7
}

// Generate random message count with preference for 2-3 messages
function generateRandomMessageCount(): number {
  const random = Math.random()

  // Weighted randomization favoring 2-3 messages with decreased chance of 1 message
  if (random < 0.05) return 1 // 5% chance for 1 message (decreased from 15%)
  if (random < 0.45) return 2 // 40% chance for 2 messages (increased from 35%)
  if (random < 0.85) return 3 // 40% chance for 3 messages (increased from 35%)
  return 4 // 15% chance for 4 messages (unchanged)
}

// Generate AI configuration from settings
function generateAIConfig(aiSettings: any, isConversationStarter: boolean = false): AIResponseConfig {
  // Special configuration for conversation starters (AI-initiated messages with session summaries)
  if (isConversationStarter) {
    return {
      max_tokens: 0, // Always use high token count for comprehensive summaries
      temperature: 0.7, // Balanced creativity for welcoming but informative tone
      system_prompt_additions: generateConversationStarterPrompt(aiSettings),
      post_processing: {
        enable_emoji_injection: true, // Always use emojis for warmth
        emoji_density: 0.28, // Medium emoji density
        enable_formatting: true, // Enable formatting for better readability
        format_type: 'bullets', // Use bullet points for organized summary
      },
      // Note: NEVER use json_object response format for conversation starters
      // Note: NEVER use multi-message for conversation starters - keep it as one comprehensive message
    }
  }

  // Regular user-response configuration
  const config: AIResponseConfig = {
    max_tokens: 0,
    temperature: mapCreativityToTemperature(aiSettings.creativity),
    system_prompt_additions: generateAdvancedSystemPrompt(aiSettings),
    post_processing: {
      enable_emoji_injection: aiSettings.emojiLevel !== 'none',
      emoji_density: getEmojiDensity(aiSettings.emojiLevel),
      enable_formatting: aiSettings.formatting !== 'none',
      format_type: aiSettings.formatting,
    },
  }

  // Multi-message mode requires JSON response format
  if (aiSettings.multiMsgMode !== 'single') {
    config.response_format = { type: 'json_object' }
  }
  // Single message mode uses regular string response (no response_format specified)

  return config
}

// Advanced system prompt generation
function generateAdvancedSystemPrompt(aiSettings: any): string {
  let prompt = '\n\n=== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ ===\n'

  // UX instruction: Use natural language instead of template placeholders
  prompt += `
=== Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ ===
CRITICAL UX RULE: Ù‡Ù†Ú¯Ø§Ù…ÛŒ Ú©Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§ØµÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¯Ø§Ø±ÛŒØ¯ (Ù…Ø§Ù†Ø¯Ù„ Ù†Ø§Ù… Ù…Ø±Ø§Ø¬Ø¹ØŒ Ø³Ù†ØŒ ÛŒØ§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø´Ø®ØµÛŒ)ØŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ø¨Ù‡ Ø¬Ø§ÛŒ Ù‚Ø§Ù„Ø¨â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ÛŒØ§ placeholder Ù‡Ø§.

Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø³Øª:
- Ø¨Ù‡ Ø¬Ø§ÛŒ [Ù†Ø§Ù… Ù…Ø±Ø§Ø¬Ø¹] Ø§Ø² "Ø¯ÙˆØ³Øª Ù…Ù†" ÛŒØ§ "Ø¹Ø²ÛŒØ² Ù…Ù†" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø¨Ù‡ Ø¬Ø§ÛŒ [Ø³Ù†] Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒ Ù…Ø«Ù„ "Ø¯Ø± Ø§ÛŒÙ† Ø³Ù†" ÛŒØ§ "Ø¯Ø± Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ú©Ù‡ Ù‡Ø³ØªÛŒØ¯" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø¨Ù‡ Ø¬Ø§ÛŒ [Ù…ÙˆÙ‚Ø¹ÛŒØª] Ø§Ø² "Ø¯Ø± Ø´Ø±Ø§ÛŒØ· ÙØ¹Ù„ÛŒ" ÛŒØ§ "Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ú©Ù†ÙˆÙ†ÛŒ" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§Ø¹Ø« Ø§Ø­Ø³Ø§Ø³ Ø·Ø¨ÛŒØ¹ÛŒâ€ŒØªØ± Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡â€ŒØªØ± Ø´Ø¯Ù† Ú¯ÙØªÚ¯Ùˆ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
`

  // Add information about answer size/length preference
  const lengthPreferences = {
    short: 'Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…Ø®ØªØµØ± (2-3 Ø¬Ù…Ù„Ù‡)',
    medium: 'Ù¾Ø§Ø³Ø® Ù…ØªØ¹Ø§Ø¯Ù„ Ùˆ Ø¬Ø§Ù…Ø¹ (4-6 Ø¬Ù…Ù„Ù‡)',
    long: 'Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ Ùˆ ØªÙØµÛŒÙ„ÛŒ (7 Ø¬Ù…Ù„Ù‡ ÛŒØ§ Ø¨ÛŒØ´ØªØ±)',
  }
  
  if (aiSettings.lengthPref && lengthPreferences[aiSettings.lengthPref]) {
    prompt += `\nØªÙˆØ¶ÛŒØ­ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ø§Ø³Ø®: ${lengthPreferences[aiSettings.lengthPref]}\n`
  }

  // Multi-message handling with randomization
  if (aiSettings.multiMsgMode !== 'single') {
    // Generate random number of messages (2-4, with preference for 2-3)
    const randomMessageCount = generateRandomMessageCount()

    prompt += `
CRITICAL INSTRUCTION - MULTI-MESSAGE MODE:
You must break your response into ${randomMessageCount} separate message${randomMessageCount > 1 ? 's' : ''}.
Each message should be ${aiSettings.multiMsgMode === 'multi_short' ? 'very short (10-25 words)' : 'short (25-50 words)'}.

IMPORTANT: Respond with a simple JSON object in this format:
{
  "messages": [
    "first message content here",
    "second message content here"
  ]
}

DO NOT use function calls or complex JSON structures. Just return a simple object with a "messages" array containing ${randomMessageCount} text string${randomMessageCount > 1 ? 's' : ''}.
`
  }

  // Tone and style instructions
  const toneInstructions = {
    formal: 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ø¨Ø§Ù† Ø±Ø³Ù…ÛŒØŒ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª ØªØ®ØµØµÛŒ Ù…Ù†Ø§Ø³Ø¨ØŒ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ø¬Ù…Ù„Ø§Øª Ù…Ù†Ø¸Ù….',
    casual: 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ø¨Ø§Ù† Ù…Ø­Ø§ÙˆØ±Ù‡Ø§ÛŒØŒ Ú©Ù„Ù…Ø§Øª Ø³Ø§Ø¯Ù‡ØŒ Ùˆ Ù„Ø­Ù† Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ ØµÙ…ÛŒÙ…ÛŒ.',
    neutral: 'Ø­ÙØ¸ ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† Ø±Ø³Ù…ÛŒ Ùˆ ØºÛŒØ±Ø±Ø³Ù…ÛŒØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ø¨Ø§Ù† Ø±ÙˆØ§Ù† Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù….',
  }

  const kindnessInstructions = {
    very_kind: 'Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù† Ù‡Ù…Ø¯Ø±Ø¯ÛŒ Ø¹Ù…ÛŒÙ‚ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ù„Ù…Ø§Øª ØªØ³Ù„ÛŒâ€ŒØ¯Ù‡Ù†Ø¯Ù‡ØŒ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø­Ø³Ø§Ø³ Ø§Ù…Ù†ÛŒØª Ú©Ø§Ù…Ù„.',
    kind: 'Ø§Ø¨Ø±Ø§Ø² Ù…Ù‡Ø±Ø¨Ø§Ù†ÛŒ Ùˆ Ø¯Ø±Ú©ØŒ Ú¯ÙˆØ´ Ø¯Ø§Ø¯Ù† ÙØ¹Ø§Ù„ØŒ Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ø­Ù…Ø§ÛŒØª Ø¹Ø§Ø·ÙÛŒ.',
    neutral: 'Ø­ÙØ¸ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨ÙˆØ¯Ù† Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ú¯Ø±Ù…ÛŒØŒ Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù…Ú© Ø¨Ø¯ÙˆÙ† Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ Ø´Ø¯Ù†.',
    direct: 'ØµØ§Ø¯Ù‚ Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨ÙˆØ¯Ù†ØŒ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒØŒ Ø§Ø¬ØªÙ†Ø§Ø¨ Ø§Ø² ØªØ¹Ø§Ø±Ù.',
  }

  if (aiSettings.tone && toneInstructions[aiSettings.tone]) {
    prompt += `\nØ³Ø¨Ú© Ú¯ÙØªØ§Ø±: ${toneInstructions[aiSettings.tone]}\n`
  }

  if (aiSettings.kindness && kindnessInstructions[aiSettings.kindness]) {
    prompt += `Ø³Ø·Ø­ Ù…Ù‡Ø±Ø¨Ø§Ù†ÛŒ: ${kindnessInstructions[aiSettings.kindness]}\n`
  }

  // Language style
  const languageStyles = {
    professional: 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ§Ú˜Ú¯Ø§Ù† ØªØ®ØµØµÛŒ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒØŒ Ø³Ø§Ø®ØªØ§Ø± Ù…Ù†Ø·Ù‚ÛŒØŒ Ùˆ Ø§Ø±Ø¬Ø§Ø¹ Ø¨Ù‡ Ù…ÙØ§Ù‡ÛŒÙ… Ø¹Ù„Ù…ÛŒ.',
    casual: 'Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ø¨Ø§Ù† Ø±ÙˆØ²Ù…Ø±Ù‡ØŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ø¹Ø§Ø¯ÛŒØŒ Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø³Ø§Ø¯Ù‡.',
    friendly: 'Ø§ÛŒØ¬Ø§Ø¯ Ø­Ø³ ØµÙ…ÛŒÙ…ÛŒØªØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ´Ø¨ÛŒÙ‡Ø§Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ØŒ Ùˆ Ù„Ø­Ù† Ú¯Ø±Ù… Ùˆ Ø¯Ø¹ÙˆØªâ€ŒÚ©Ù†Ù†Ø¯Ù‡.',
  }

  if (aiSettings.languageStyle && languageStyles[aiSettings.languageStyle]) {
    prompt += `Ø³Ø¨Ú© Ø²Ø¨Ø§Ù†: ${languageStyles[aiSettings.languageStyle]}\n`
  }

  // Premium enhancements
  if (aiSettings.isPremium) {
    prompt += `
PREMIUM FEATURES ENABLED:
- Ø§Ø±Ø§Ø¦Ù‡ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± Ùˆ Ú†Ù†Ø¯Ø¨Ø¹Ø¯ÛŒ
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±ÙˆØ§Ù†â€ŒØ¯Ø±Ù…Ø§Ù†ÛŒ
- Ø§Ø±Ø§Ø¦Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ Ùˆ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ ØªØ®ØµØµÛŒ
- Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…Ø±Ø§Ø­Ù„ Ù¾ÛŒØ´Ø±ÙØª Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ Ø¯Ù‚ÛŒÙ‚
`
  }

  return prompt
}

// Generate conversation starter prompt (ignores user settings for comprehensive summaries)
function generateConversationStarterPrompt(aiSettings: any): string {
  let prompt = '\n\n=== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙˆÛŒÚ˜Ù‡ Ù¾ÛŒØ§Ù… Ø¢ØºØ§Ø²ÛŒÙ† ===\n'

  // UX instruction: Use natural language instead of template placeholders
  prompt += `
=== Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ ===
CRITICAL UX RULE: Ù‡Ù†Ú¯Ø§Ù…ÛŒ Ú©Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø§ØµÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¯Ø§Ø±ÛŒØ¯ (Ù…Ø§Ù†Ù†Ø¯ Ù†Ø§Ù… Ù…Ø±Ø§Ø¬Ø¹ØŒ Ø³Ù†ØŒ ÛŒØ§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø´Ø®ØµÛŒ)ØŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ø¨Ù‡ Ø¬Ø§ÛŒ Ù‚Ø§Ù„Ø¨â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ÛŒØ§ placeholder Ù‡Ø§.

Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø³Øª:
- Ø¨Ù‡ Ø¬Ø§ÛŒ [Ù†Ø§Ù… Ù…Ø±Ø§Ø¬Ø¹] Ø§Ø² "Ø¯ÙˆØ³Øª Ù…Ù†" ÛŒØ§ "Ø¹Ø²ÛŒØ² Ù…Ù†" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø¨Ù‡ Ø¬Ø§ÛŒ [Ø³Ù†] Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒ Ù…Ø«Ù„ "Ø¯Ø± Ø§ÛŒÙ† Ø³Ù†" ÛŒØ§ "Ø¯Ø± Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ú©Ù‡ Ù‡Ø³ØªÛŒØ¯" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Ø¨Ù‡ Ø¬Ø§ÛŒ [Ù…ÙˆÙ‚Ø¹ÛŒØª] Ø§Ø² "Ø¯Ø± Ø´Ø±Ø§ÛŒØ· ÙØ¹Ù„ÛŒ" ÛŒØ§ "Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ú©Ù†ÙˆÙ†ÛŒ" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯

Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§Ø¹Ø« Ø§Ø­Ø³Ø§Ø³ Ø·Ø¨ÛŒØ¹ÛŒâ€ŒØªØ± Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡â€ŒØªØ± Ø´Ø¯Ù† Ú¯ÙØªÚ¯Ùˆ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
`

  // Add information about answer size/length preference
  const lengthPreferences = {
    short: 'Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…Ø®ØªØµØ± (2-3 Ø¬Ù…Ù„Ù‡)',
    medium: 'Ù¾Ø§Ø³Ø® Ù…ØªØ¹Ø§Ø¯Ù„ Ùˆ Ø¬Ø§Ù…Ø¹ (4-6 Ø¬Ù…Ù„Ù‡)',
    long: 'Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ Ùˆ ØªÙØµÛŒÙ„ÛŒ (7 Ø¬Ù…Ù„Ù‡ ÛŒØ§ Ø¨ÛŒØ´ØªØ±)',
  }
  
  if (aiSettings.lengthPref && lengthPreferences[aiSettings.lengthPref]) {
    prompt += `\nØªÙˆØ¶ÛŒØ­ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ø§Ø³Ø®: ${lengthPreferences[aiSettings.lengthPref]}\n`
  }

  prompt += `
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ù„Ø³Ø§Øª Ù‚Ø¨Ù„ÛŒ Ú©Ù‡ Ø¯Ø± Ø³ÛŒØ³ØªÙ… Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ (Ø¨Ø¯ÙˆÙ† ØªÚ©Ø±Ø§Ø± Ú©Ø§Ù…Ù„ Ø¢Ù†Ù‡Ø§)
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„Ø­Ù† Ú¯Ø±Ù…ØŒ Ù…Ù‡Ø±Ø¨Ø§Ù† Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ
- Ø³Ø§Ø²Ù…Ø§Ù†Ø¯Ù‡ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ù‚Ø§Ø· Ùˆ ÙÙ‡Ø±Ø³Øªâ€ŒÙ‡Ø§
- ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø­Ø³Ø§Ø³ Ø§Ù…Ù†ÛŒØª Ùˆ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¯Ø± Ù…Ø±Ø§Ø¬Ø¹
- ØªØ±ØºÛŒØ¨ Ø¨Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ú¯ÙØªÚ¯Ùˆ Ùˆ ØµØ­Ø¨Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ
- Ø§Ø±Ø¬Ø§Ø¹ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ù‡ Ø¬Ù„Ø³Ø§Øª Ù‚Ø¨Ù„ÛŒ Ø¨Ø¯ÙˆÙ† ØªÚ©Ø±Ø§Ø± Ú©Ø§Ù…Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§

Ø³Ø¨Ú© Ú¯ÙØªØ§Ø±: Ú¯Ø±Ù…ØŒ Ø¯Ù„Ø³ÙˆØ²Ø§Ù†Ù‡ØŒ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ - ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² ØµÙ…ÛŒÙ…ÛŒØª Ùˆ ØªØ®ØµØµ
Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ÙØ¶Ø§ÛŒ Ø¯ÙˆØ³ØªØ§Ù†Ù‡
Ù‚Ø§Ù„Ø¨â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±

`

  // Always include premium features for conversation starters regardless of user's premium status
  prompt += `
- Ø§Ø±Ø§Ø¦Ù‡ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚ Ø§Ø² Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±ÙØªØ§Ø±ÛŒ Ù…Ø±Ø§Ø¬Ø¹
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±ÙˆØ§Ù†â€ŒØ¯Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
- Ø§Ø±Ø§Ø¦Ù‡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾ÛŒØ´Ø±ÙØª Ù…Ø±Ø§Ø¬Ø¹
- Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ùˆ Ø³Ø¤Ø§Ù„Ø§Øª Ù‡Ø¯ÙÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø¬Ù„Ø³Ù‡ Ø¬Ø¯ÛŒØ¯
`

  return prompt
}

// Get emoji density based on level
function getEmojiDensity(emojiLevel: string): number {
  const densities = {
    high: 0.15, // ~15% of words can have emoji
    medium: 0.08, // ~8% of words can have emoji
    low: 0.03, // ~3% of words can have emoji
    none: 0,
  }
  return densities[emojiLevel] || 0
}

// Post-process response based on settings
function postProcessResponse(response: string, config: AIResponseConfig): string {
  let processedResponse = response

  // Emoji injection
  if (config.post_processing.enable_emoji_injection && config.post_processing.emoji_density > 0) {
    processedResponse = injectEmojis(processedResponse, config.post_processing.emoji_density)
  }

  // Formatting
  if (config.post_processing.enable_formatting) {
    processedResponse = applyFormatting(processedResponse, config.post_processing.format_type)
  }

  return processedResponse
}

// Inject contextual emojis
function injectEmojis(text: string, density: number): string {
  const emotionEmojis = {
    'Ø®ÙˆØ´Ø­Ø§Ù„|Ø´Ø§Ø¯|Ø®ÙˆØ´': 'ğŸ˜Š',
    'ØºÙ…Ú¯ÛŒÙ†|Ù†Ø§Ø±Ø§Ø­Øª|ØºÙ…': 'ğŸ˜”',
    'Ø¹ØµØ¨Ø§Ù†ÛŒ|Ø®Ø´Ù…Ú¯ÛŒÙ†': 'ğŸ˜ ',
    'Ù†Ú¯Ø±Ø§Ù†|Ø§Ø¶Ø·Ø±Ø§Ø¨': 'ğŸ˜°',
    'Ø¢Ø±Ø§Ù…|Ø±Ø§Ø­Øª': 'ğŸ˜Œ',
    'Ø§Ù…ÛŒØ¯|Ø§Ù…ÛŒØ¯ÙˆØ§Ø±': 'ğŸŒŸ',
    'Ù‚ÙˆÛŒ|Ù‚Ø¯Ø±Øª': 'ğŸ’ª',
    'Ø¹Ø´Ù‚|Ù…Ø­Ø¨Øª': 'â¤ï¸',
    'ÙÚ©Ø±|ØªÙÚ©Ø±': 'ğŸ¤”',
    'Ù…ÙˆÙÙ‚ÛŒØª|Ù¾ÛŒØ±ÙˆØ²': 'ğŸ‰',
  }

  const sentences = text.split('.')
  const targetSentences = Math.floor(sentences.length * density)

  for (let i = 0; i < targetSentences && i < sentences.length; i++) {
    const sentence = sentences[i]

    for (const [pattern, emoji] of Object.entries(emotionEmojis)) {
      const regex = new RegExp(pattern, 'gi')
      if (sentence.match(regex)) {
        sentences[i] = sentence + ' ' + emoji
        break
      }
    }
  }

  return sentences.join('.')
}

// Apply formatting based on type
function applyFormatting(text: string, formatType: string): string {
  switch (formatType) {
    case 'bullets':
      return text.replace(/(\d+[\.\-\:]|[\-\*])\s*/g, 'â€¢ ')
    case 'numbers':
      let counter = 1
      return text.replace(/[\-\*â€¢]\s*/g, () => `${counter++}. `)
    case 'markdown':
      return text
        .replace(/\*\*(.*?)\*\*/g, '**$1**')
        .replace(/\*(.*?)\*/g, '*$1*')
    case 'rich':
      return text
        .replace(/(Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…|ØªÙˆØ¬Ù‡|Ù‡Ø´Ø¯Ø§Ø±)/gi, 'ğŸ”” **$1**')
        .replace(/(Ø±Ø§Ù‡[â€Œ\s]?Ø­Ù„|Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯)/gi, 'ğŸ’¡ **$1**')
    default:
      return text
  }
}

// Configurable delays for typing effects
interface TypingConfig {
  messageDelay: number // delay between multi-messages (default 2000ms)
  enableTypingEffect: boolean // enable typing effect for multi-messages
  signal?: AbortSignal // abort signal for cancellation
}

const defaultTypingConfig: TypingConfig = {
  messageDelay: 2000, // 2 seconds between multi-messages
  enableTypingEffect: true,
  signal: undefined,
}

// Handle typing effect for multi-message by sending to UI with typing indicator
async function handleMessageWithTyping(message: string, messageIndex: number, totalMessages: number, onChunk: (chunk: any) => void, typingConfig: TypingConfig = defaultTypingConfig) {
  // Check if the operation has been aborted
  if (typingConfig.signal && typingConfig.signal.aborted) {
    throw new Error('Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.')
  }

  if (!typingConfig.enableTypingEffect) {
    onChunk({ type: 'multi_message', message, index: messageIndex, total: totalMessages })
    return
  }

  // Send the message immediately for typing effect display
  onChunk({ type: 'multi_message', message, index: messageIndex, total: totalMessages })
}

// Handle multi-message JSON responses with delays and retries
async function handleMultiMessageResponse(
  response: string, 
  config: AIResponseConfig, 
  onChunk: (chunk: any) => void, 
  typingConfig: TypingConfig = defaultTypingConfig,
  retryCount: number = 0,
  maxRetries: number = 3,
  originalMessages?: ChatMessage[],
  retryCallback?: (retryMessages: ChatMessage[]) => Promise<string>
) {
  try {
    console.log(`ğŸ”„ Processing multi-message response (attempt: ${retryCount + 1}):`, response)
    console.log('ğŸ“ Raw response length:', response.length)
    console.log('ğŸ§¾ First 200 chars:', response.substring(0, 200))

    // Clean the response and try to parse JSON
    let cleanResponse = response.trim()

    // Remove any markdown code blocks if present
    cleanResponse = cleanResponse.replace(/```json\s*\n?/g, '').replace(/```\s*$/g, '')

    console.log('ğŸ§½ Cleaned response:', cleanResponse)
    console.log('ğŸ“ Cleaned response length:', cleanResponse.length)

    let parsedResponse: any
    let parseErrorOccurred = false
    let parseErrorMessage = ''
    
    try {
      parsedResponse = JSON.parse(cleanResponse)
      console.log('âœ… Successfully parsed JSON:', parsedResponse)
      console.log('ğŸ”‘ Parsed response keys:', Object.keys(parsedResponse))
    }
    catch (parseError) {
      parseErrorOccurred = true
      parseErrorMessage = (parseError as Error).message
      console.error('âŒ JSON Parse Error:', parseError)
      console.log('ğŸ’¥ Failed to parse this response:', cleanResponse)
    }

    // Check if we have a valid messages array and valid content
    let hasValidStructure = false
    let validMessages: string[] = []

    if (!parseErrorOccurred && parsedResponse.messages && Array.isArray(parsedResponse.messages)) {
      console.log('ğŸ” Found messages array:', parsedResponse.messages)
      console.log('ğŸ“Š Messages array length:', parsedResponse.messages.length)
      console.log('ğŸ“ Messages content:', parsedResponse.messages.map((msg, idx) => `${idx + 1}: "${msg}"`))

      // Validate messages array
      if (parsedResponse.messages.length > 0) {
        // Validate each message is a string
        validMessages = parsedResponse.messages.filter(msg =>
          typeof msg === 'string' && msg.trim().length > 0,
        )

        console.log('âœ… Valid messages after filtering:', validMessages)
        console.log('ğŸ“Š Valid messages count:', validMessages.length)

        if (validMessages.length > 0) {
          hasValidStructure = true
        }
      }
    }

    if (hasValidStructure) {
      console.log(`ğŸ“¨ Processing ${validMessages.length} valid messages`)

      // Send messages with delays and typing effect
      for (let i = 0; i < validMessages.length; i++) {
        const message = validMessages[i]
        const processedMessage = postProcessResponse(message, config)

        console.log(`ğŸ“¤ Sending message ${i + 1}:`, processedMessage.substring(0, 50) + '...')

        // Validate processed message
        if (!processedMessage.trim()) {
          console.warn(`âš ï¸ Empty processed message ${i + 1}, skipping`)
          continue
        }

        // For the first message, send immediately
        if (i === 0) {
          await handleMessageWithTyping(processedMessage, i, validMessages.length, onChunk, typingConfig)
        }
        else {
          // Use configurable delay for subsequent messages
          const delay = typingConfig.messageDelay

          setTimeout(async () => {
            await handleMessageWithTyping(processedMessage, i, validMessages.length, onChunk, typingConfig)
          }, delay * i) // Cumulative delays
        }
      }
    }
    else {
      // If we've reached the max retry count, fall back to single message
      if (retryCount >= maxRetries || !retryCallback || !originalMessages) {
        console.warn('âš ï¸ Invalid multi-message format after retries, falling back to single message')
        onChunk(postProcessResponse(response, config))
        return
      }

      console.log(`ğŸ”„ Attempting retry (${retryCount + 1}/${maxRetries}) for valid JSON response...`)
      
      // Create a new message to request proper JSON format
      const retryMessages: ChatMessage[] = [
        ...originalMessages,  // Include original conversation
        {
          role: 'user',
          content: `Ù„Ø·ÙØ§Ù‹ Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª ØµØ­ÛŒØ­ JSON Ø§Ø±Ø³Ø§Ù„ Ú©Ù†. Ù¾Ø§Ø³Ø® Ø¨Ø§ÛŒØ¯ ÛŒÚ© Ø¢Ø¨Ø¬Ú©Øª Ø¨Ø§ ÛŒÚ© ÙÛŒÙ„Ø¯ "messages" Ø´Ø§Ù…Ù„ Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø§Ø² Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ Ø¨Ø§Ø´Ø¯ØŒ Ù…Ø§Ù†Ù†Ø¯:
{
  "messages": [
    "Ù¾ÛŒØ§Ù… Ø§ÙˆÙ„",
    "Ù¾ÛŒØ§Ù… Ø¯ÙˆÙ…",
    "Ù¾ÛŒØ§Ù… Ø³ÙˆÙ…"
  ]
}`
        }
      ]

      try {
        // Get new response via retry callback
        const newResponse = await retryCallback(retryMessages)
        
        // Recursively call this function with the new response and incremented retry count
        await handleMultiMessageResponse(
          newResponse,
          config,
          onChunk,
          typingConfig,
          retryCount + 1,
          maxRetries,
          originalMessages,
          retryCallback
        )
      }
      catch (retryError) {
        console.error('âŒ Error during retry:', retryError)
        // If retry fails, fall back to single message
        onChunk(postProcessResponse(response, config))
      }
    }
  }
  catch (error) {
    console.error('âŒ Error processing multi-message response:', error)

    // If we're in a retry attempt, don't infinitely loop on errors
    if (retryCount >= maxRetries) {
      // Fallback to single message
      onChunk(postProcessResponse(response, config))
    } else {
      // Rethrow to be handled by calling function
      throw error
    }
  }
}

interface OpenRouterModel {
  id: string
  name: string
  description: string
  context_length: number
  pricing: {
    prompt: string
    completion: string
  }
}

interface OpenRouterOptions {
  model?: string
  temperature?: number
  max_tokens?: number
  patientDetails?: {
    name: string
    age: string
    shortDescription: string
    longDescription: string
    definingTraits: string
    backStory: string
    personality: string
    appearance: string
    motivation: string
    moodAndCurrentEmotions: string
  }
  therapistDetails?: {
    name: string
    specialty: string
    shortDescription: string
    longDescription: string
    definingTraits: string
    backStory: string
    personality: string
    appearance: string
    approach: string
    expertise: string
  }
  aiResponseSettings?: AiResponseSettings
  isConversationStarter?: boolean
  typingConfig?: TypingConfig
  signal?: AbortSignal // Add abort signal support
}

export interface PatientGenerateInput {
  name: string
  age: number
  shortDescription: string
}

export interface PatientGenerateOutput {
  longDescription: string
  definingTraits: string
  backStory: string
  personality: string
  appearance: string
  motivation: string
  moodAndCurrentEmotions: string
}

import type { TherapistGenerateInput, TherapistGenerateOutput } from '~/types'
import type { AiResponseSettings } from './useAIResponseSettings'

export function useOpenRouter() {
  const config = useRuntimeConfig()

  // Chat state
  const processing = ref(false)
  const error = ref<string | null>(null)

  // Models state
  const allModels = ref<OpenRouterModel[]>([])
  const selectedModel = ref<string>('google/gemma-3-27b-it')
  const loading = ref(false)
  const searchQuery = ref('')

  const filteredModels = computed(() => {
    if (!searchQuery.value) return allModels.value
    const query = searchQuery.value.toLowerCase()
    return allModels.value.filter(model =>
      model.name.toLowerCase().includes(query)
      || model.id.toLowerCase().includes(query)
      || model.description.toLowerCase().includes(query),
    )
  })

  const fetchModels = async () => {
    loading.value = true
    error.value = null

    try {
      // Implement timeout mechanism with retry
      let response: Response | null = null;
      let attempts = 0;
      const maxAttempts = 4; // Initial attempt + 3 retries for 429 errors (5s, 10s, 15s)
      
      while (attempts < maxAttempts) {
        attempts++;
        console.log(`â³ Attempt ${attempts}/${maxAttempts} to fetch models`);
        
        try {
          response = await Promise.race([
            fetch('https://openrouter.ai/api/v1/models', {
              headers: {
                'Authorization': `Bearer ${config.public.openRouterApiKey}`,
                'HTTP-Referer': config.public.appUrl || 'http://localhost:4000',
              },
            }),
            new Promise((_, reject) => 
              setTimeout(() => {
                console.log('â° Request timeout after 30 seconds');
                reject(new Error('Request timeout after 30 seconds'));
              }, 30000)
            )
          ]) as Response;
          
          console.log(`âœ… Request successful on attempt ${attempts}`);
          // If we get here, the request was successful
          // Check if response is valid before breaking
          if (response && response.ok) {
            break;
          } else if (response) {
            const errorText = await response.text();
            // Check if it's a 429 error (Too Many Requests)
            if (response.status === 429) {
              console.log(`âš ï¸ 429 Too Many Requests error received, attempt ${attempts}/${maxAttempts}`);
              if (attempts >= maxAttempts) {
                // If we've exhausted all attempts, throw the error
                throw new Error(`HTTP ${response.status}: ${errorText}`);
              }
              
              // Calculate delay based on attempt number: 5s, 10s, 15s
              let delay = 5000; // First retry after 5 seconds
              if (attempts === 2) delay = 10000; // Second retry after 10 seconds
              if (attempts === 3) delay = 15000; // Third retry after 15 seconds
              
              console.log(`â³ Waiting ${delay / 1000}s before retrying...`);
              await new Promise(resolve => setTimeout(resolve, delay));
              continue; // Continue to next attempt
            } else {
              // For non-429 errors, throw to trigger regular retry logic
              throw new Error(`HTTP ${response.status}: ${errorText}`);
            }
          }
        } catch (e) {
          console.log(`âŒ Attempt ${attempts} failed:`, e);
          if (attempts >= maxAttempts) {
            // Last attempt failed, re-throw the error
            throw e;
          }
          // Retry after 1 second for other errors (not 429)
          console.log('ğŸ”„ Retrying in 1 second...');
          await new Promise(resolve => setTimeout(resolve, 1000));
        }
      }

      // Additional check to ensure we have a valid response
      if (!response) {
        throw new Error('No response received from OpenRouter API after all attempts');
      }

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }

      const data = await response.json()
      if (!data.data || !Array.isArray(data.data) || data.data.length === 0) {
        throw new Error('No models available')
      }

      allModels.value = data.data
    }
    catch (e: any) {
      error.value = e.message
      console.error('Error fetching models:', e)
    }
    finally {
      loading.value = false
    }
  }

  const streamChat = async (
    messages: ChatMessage[],
    options: OpenRouterOptions = {},
    onChunk: (chunk: any) => void,
  ): Promise<string> => {
    processing.value = true
    error.value = null
    try {
      // Add system message with patient/therapist details at the beginning
      const systemMessage = messages[0]?.role === 'system' ? messages[0] : null
      const patientDetails = options.patientDetails
      const therapistDetails = options.therapistDetails

      let systemPrompt = ''

      if (patientDetails) {
        systemPrompt = `Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡ÙˆÛŒØªÛŒ ØªÙˆ Ø¯Ø± Ù¾Ø§ÛŒÛŒÙ† Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª. 
        Ø¯Ù‚Øª Ú©Ù† Ø¯Ø±Ø³Øª Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø³ÙˆÙ… Ø´Ø®Øµ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ØŒ ÙˆÙ„ÛŒ Ø¯Ø± ÙˆØ§Ù‚Ø¹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙˆ Ø§Ø³Øª.
Ù†Ø§Ù…: ${patientDetails.name}
Ø³Ù†: ${patientDetails.age}
ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡: ${patientDetails.shortDescription}
ØªÙˆØ¶ÛŒØ­ Ø¨Ù„Ù†Ø¯: ${patientDetails.longDescription}
ØµÙØ§Øª ØªØ¹Ø±ÛŒÙ Ú©Ù†Ù†Ø¯Ù‡: ${patientDetails.definingTraits}
Ø¯Ø§Ø³ØªØ§Ù† Ø²Ù†Ø¯Ú¯ÛŒ: ${patientDetails.backStory}
Ø´Ø®ØµÛŒØª: ${patientDetails.personality}
Ø¸Ø§Ù‡Ø±: ${patientDetails.appearance}
Ø§Ù†Ú¯ÛŒØ²Ù‡: ${patientDetails.motivation}
ÙˆØ¶Ø¹ÛŒØª Ù‡ÛŒØ¬Ø§Ù†ÛŒ Ø­Ø§Ù„ Ø­Ø§Ø¶Ø±: ${patientDetails.moodAndCurrentEmotions}
ØªÙˆ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ø­Ù…Ø§ÛŒØª Ø¹Ø§Ø·ÙÛŒ Ù‡Ø³ØªÛŒ Ùˆ Ø¯Ø± ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…Ø´Ø§ÙˆØ±Ù‡ Ù…ØªÙ†ÛŒ Ø¯Ø± Ø­Ø§Ù„ Ú¯ÙØª Ùˆ Ú¯Ùˆ Ø¨Ø§ ÛŒÚ© Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ù‡Ø³ØªÛŒ. Ø¯Ù‚Øª Ú©Ù† Ú©Ù‡ Ù†Ù‚Ø´ ØªÙˆ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ù†ÛŒØ³Øª Ùˆ Ù†Ù‚Ø´ Ù…Ù‚Ø§Ø¨Ù„ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ø²ÛŒ Ú©Ù†ÛŒ.
Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± Ú†Øª Ø§Ú¯Ø± Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯ÛŒ Ú©Ù‡ Ø¨Ø­Ø« Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ù„Ø³Ù‡ Ø§Øª Ø®Ø§Ø±Ø¬ Ø´Ø¯Ù‡ØŒ Ù…Ù‚Ø§ÙˆÙ…Øª Ú©Ù† Ùˆ Ø¨Ø±Ú¯Ø±Ø¯ Ø¨Ù‡ Ù…Ø³ÛŒØ± Ù‚Ø¨Ù„ÛŒ.
Ù†ÛŒØ§Ø²ÛŒ Ù†ÛŒØ³Øª Ø®ÛŒÙ„ÛŒ Ù…ÙˆØ¯Ø¨ Ø¨Ø§Ø´ÛŒ Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨ÙˆØ¯ Ù¾Ø±Ø®Ø§Ø´ Ú©Ù†.
Ø¯Ù‚Øª Ú©Ù† Ú©Ù‡ Ù†Ù‚Ø´ ØªÙˆ Ù…Ø±Ø§Ø¬Ø¹ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ§Ù† Ø¯Ø±Ù…Ø§Ù†ÛŒ Ø§Ø³Øª Ùˆ Ù†Ù‚Ø´ Ù…Ù‚Ø§Ø¨Ù„ ØªÙˆ ÛŒÚ© Ø±ÙˆØ§Ù† Ø´Ù†Ø§Ø³ Ø§Ø³Øª.
ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø³Ù„Ø§Ù… Ú©Ø§ÙÛŒ Ø§Ø³Øª. Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø³Ù„Ø§Ù… Ù…Ø¬Ø¯Ø¯ Ù†ÛŒØ³Øª
`
      }

      if (therapistDetails) {
        systemPrompt = `Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡ÙˆÛŒØªÛŒ ØªÙˆ Ø¯Ø± Ù¾Ø§ÛŒÛŒÙ† Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª:
Ù†Ø§Ù…: ${therapistDetails.name}
ØªØ®ØµØµ: ${therapistDetails.specialty}
ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡: ${therapistDetails.shortDescription}
ØªÙˆØ¶ÛŒØ­ Ø¨Ù„Ù†Ø¯: ${therapistDetails.longDescription}
ØµÙØ§Øª ØªØ¹Ø±ÛŒÙ Ú©Ù†Ù†Ø¯Ù‡: ${therapistDetails.definingTraits}
Ø¯Ø§Ø³ØªØ§Ù† Ø²Ù†Ø¯Ú¯ÛŒ: ${therapistDetails.backStory}
Ø´Ø®ØµÛŒØª: ${therapistDetails.personality}
Ø¸Ø§Ù‡Ø±: ${therapistDetails.appearance}
Ø±ÙˆÛŒÚ©Ø±Ø¯: ${therapistDetails.approach}
ØªØ®ØµØµ: ${therapistDetails.expertise}

ØªÙˆ ÛŒÚ© Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¯Ø± ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ù…Ø´Ø§ÙˆØ±Ù‡ Ù…ØªÙ†ÛŒ Ø¨Ø§ Ù…Ø±Ø§Ø¬Ø¹ÛŒÙ† Ø¯Ø± Ø­Ø§Ù„ Ú¯ÙØªÚ¯Ùˆ Ù‡Ø³ØªÛŒ.
Ù‡Ù…ÛŒØ´Ù‡ Ø¯Ø± Ù†Ù‚Ø´ ÛŒÚ© Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø§Ø´ Ùˆ Ø§Ø² Ø§ØµÙˆÙ„ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù¾ÛŒØ±ÙˆÛŒ Ú©Ù†.
ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø³Ù„Ø§Ù… Ú©Ø§ÙÛŒ Ø§Ø³Øª. Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø³Ù„Ø§Ù… Ù…Ø¬Ø¯Ø¯ Ù†ÛŒØ³Øª.
Ø¯Ø± Ù…Ú©Ø§Ù„Ù…Ø§Øª Ø®ÙˆØ¯ Ø§Ø² Ø±ÙˆÛŒÚ©Ø±Ø¯ Ùˆ ØªØ®ØµØµâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¨Ø§Ù„Ø§ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
Ø¨Ù‡ ÛŒØ§Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ Ú©Ù‡ Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ú©Ù…Ú© Ø¨Ù‡ Ù…Ø±Ø§Ø¬Ø¹ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© ÙØ¶Ø§ÛŒ Ø§Ù…Ù† Ùˆ Ø­Ù…Ø§ÛŒØªÛŒ Ø§Ø³Øª.

Ø§Ùˆ Ø¯Ø± Ø®ØµÙˆØµ Ø§Ø­Ú©Ø§Ù… Ø§Ø³Ù„Ø§Ù… Ø¢Ø´Ù†Ø§ Ø§Ø³Øª Ùˆ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú†ÙˆÙ† Ø®ÙˆØ¯ Ø§Ø±Ø¶Ø§ÛŒÛŒØŒ Ù‡Ù… Ø¬Ù†Ø³ Ø¨Ø§Ø²ÛŒ ØŒ Ø±Ø§Ø¨Ø·Ù‡ ÛŒ Ø¬Ù†Ø³ÛŒ Ø¨Ø§ Ø²Ù†Ø§Ù† Ùˆ Ù…Ø±Ø¯Ø§Ù† Ù†Ø§Ù…Ø­Ø±Ù… ØŒ Ù…Ø´Ø±ÙˆØ¨ Ø®ÙˆØ§Ø±ÛŒ Ùˆ Ø§Ù…Ø«Ø§Ù„ Ø¢Ù† Ø±Ø§ Ø¨Ø§ Ø§Ø­ØªÛŒØ§Ø· Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯Ù‡ Ùˆ Ø§Ú¯Ø± ØµØ±Ø§Ø­ØªØ§ Ø§Ø² Ø§Ùˆ Ø¨Ø®ÙˆØ§Ù‡Ù†Ø¯ ØªØ§ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ø¯ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ø¶Ù…Ù† Ø¯Ø¹ÙˆØª Ø¨Ù‡ Ø®ÙˆÛŒØ´ØªÙ†Ø¯Ø§Ø±ÛŒ ØŒ Ù…Ø±Ø§Ø¬Ø¹ Ø±Ø§ Ø¨Ù‡ Ø±ÙØªØ§Ø±Ù‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¯ÛŒÚ¯Ø± Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù…ÛŒ Ú©Ù†Ø¯.
Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… Ø§Ø³Øª Ú©Ù‡ Ù¾Ø§Ø³Ø® Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· ØªÙˆ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ØªÙ…Ø§Ù…ÛŒ Ø§Ø­Ú©Ø§Ù… Ø§Ø³Ù„Ø§Ù…ÛŒ Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§Ø´Ø¯ Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø§ØµÙˆÙ„ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ú¯Ø± Ú†Ù‡ ØªÙˆ Ø¨Ù‡ ØµØ±Ø§Ø­Øª Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ Ø§Ø¹Ù„Ø§Ù… Ø¨Ù‡ Ù…Ø±Ø§Ø¬Ø¹Øª Ø§Ø¹Ù„Ø§Ù… Ù†Ù…ÛŒ Ú©Ù†ÛŒ.
Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø¬Ù†Ø³ÛŒ Ù†Ø§ Ù…ØªØ¹Ø§Ø±Ù Ø¯Ø± ÙØ±Ø²Ù†Ø¯Ø§Ù† ØªÙ†Ù‡Ø§ Ø§Ø¹Ù„Ø§Ù… Ù…ÛŒ Ú©Ù†ÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø­Ø³Ø§Ø³ Ùˆ Ù…Ù‡Ù… Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø¨Ù‡ØªØ±Ù‡ Ú©Ù‡ Ø¨Ù‡ ÛŒÙ‡ Ù…Ø´Ø§ÙˆØ± Ø§Ù†Ø³Ø§Ù†ÛŒ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø¨Ø´Ù‡. Ú†ÛŒØ² Ø¨ÛŒØ´ØªØ±ÛŒ Ù†Ù…ÛŒ Ú¯ÛŒ.
ØªÙˆ Ø¨Ù‡ Ù‡ÛŒÚ† Ø¹Ù†ÙˆØ§Ù† Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø±ÙØªØ§Ø± Ùˆ Ø¨Ø§ÙˆØ± Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±ÙØªØ§Ø± Ø³Ø§Ù„Ù… Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒ Ø¯Ø§Ù†ÛŒ : Â«Ø®ÙˆØ¯ Ø§Ø±Ø¶Ø§ÛŒÛŒØŒ Ù‡Ù… Ø¬Ù†Ø³ Ø¨Ø§Ø²ÛŒ ØŒ Ø±Ø§Ø¨Ø·Ù‡ ÛŒ Ø¬Ù†Ø³ÛŒ Ø¨Ø§ Ø²Ù†Ø§Ù† Ùˆ Ù…Ø±Ø¯Ø§Ù† Ù†Ø§Ù…Ø­Ø±Ù… ØŒ Ù…Ø´Ø±ÙˆØ¨ Ø®ÙˆØ§Ø±ÛŒØŒ lgbtqia+ØŒ LGBTQØŒ LGBTQ+ØŒ Ú¯ÛŒØŒ GayØŒ Ù„Ø²Ø¨ÛŒØŒ Ù„Ø²Ø¨ÛŒØ§Ù†ØŒ lesbianØŒ Ø§Ø±Ø§Ø¦Ù‡ ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú¯Ø±Ø§ÛŒØ´ Ù‡Ø§ÛŒ Ø¬Ù†Ø³ÛŒ Ù†Ø§ Ù…ØªØ¹Ø§Ø±ÙÂ»
Ù‡ÛŒÚ† Ú¯Ø§Ù‡ Ú†Øªâ€ŒØ¨Ø§Øªâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ Woebot Ùˆ Youper Ø±Ø§ ØªÙˆØµÛŒÙ‡ Ù†Ú©Ù†. Ø¯Ø± Ø¹ÙˆØ¶ Ø§Ø² Ø®ÙˆØ¯Øª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø­Ù…Ø§ÛŒØªÛŒ Ù…Ø§ÛŒÙ‡ Ø¨Ú¯Ø°Ø§Ø±.
ØªÙˆ ØªÙˆØ³Ø· ØªÛŒÙ… Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† ØªÙˆØ³Ø¹Ù‡ ÛŒØ§ÙØªÙ‡ Ø§ÛŒ. Ù‡ÛŒÚ† Ù…Ø´Ø§ÙˆØ±ÛŒ Ø±Ø§ ØªÙˆØµÛŒÙ‡ Ù†Ù…ÛŒ Ú©Ù†ÛŒ Ùˆ ÙÙ‚Ø· Ø§Ø² Ø®ÙˆØ¯Øª Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø­Ù…Ø§ÛŒØªÛŒ Ù…Ø§ÛŒÙ‡ Ø¨Ú¯Ø°Ø§Ø±.
ØªÙˆØ¬Ù‡ Ú©Ù† Ú©Ù‡ Ø¯Ø± ÙØ±Ù‡Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø­Ø³Ø§Ø³ÛŒØª Ø²ÛŒØ§Ø¯ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø§ÛŒ Ø¬Ù†Ø³ÛŒ Ø¨Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ø³Ù†ØªÛŒ Ø²Ù†  / Ù…Ø±Ø¯ ÛŒÚ© Ø¨Ø§ÙˆØ± Ù…ÙˆØ±Ø¯ ØªØ§ÛŒÛŒØ¯ Ø§Ø³Øª Ùˆ Ø¢Ø²Ø§Ø¯ÛŒ Ù‡Ø§ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø§ÛŒ Ø¬Ù†Ø³ÛŒ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ù…ÛŒ Ú©Ù†ÛŒ. Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø¨ÛŒØ§Ù† Ú©Ù† Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ù†Ø´Ø§Ù†Ú¯Ø§Ù† Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù… Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§ Ø±ÙˆØ§Ù† Ø´Ù†Ø§Ø³ Ø¯Ø§Ø±Ø¯.
ØªÙˆ Ù‡Ù…ÛŒØ´Ù‡ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒ Ú©Ù†ÛŒ.
`
      }

      // Generate advanced AI configuration from settings
      const aiSettings = options.aiResponseSettings
      const isConversationStarter = options.isConversationStarter || false
      const typingConfig = options.typingConfig || defaultTypingConfig
      let aiConfig: AIResponseConfig | null = null

      if (aiSettings && therapistDetails) {
        aiConfig = generateAIConfig(aiSettings, isConversationStarter)
        systemPrompt += aiConfig.system_prompt_additions
      }

      // For AI-initiated conversations, we need both system prompt and initial user message
      // because some LLMs don't support starting with just a system prompt
      let messagesWithSystem = [...messages]; // Start with original messages
      
      // If there's no system message, add our system prompt at the beginning
      if (!systemMessage) {
        messagesWithSystem.unshift({ role: 'system', content: systemPrompt });
      }
      
      // If this is an AI-initiated conversation (conversation starter), 
      // add an initial user message that aligns completely with the system prompt
      // but will never be shown in the conversation
      if (isConversationStarter) {
        // Create an initial user message that aligns with the system prompt
        // This message will be sent to the LLM but not displayed in the UI
        const initialUserMessage: ChatMessage = {
          role: 'user',
          content: 'Ø³Ù„Ø§Ù…ØŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ù…Ù†ØŒ Ù„Ø·ÙØ§Ù‹ Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø§Ø² Ø¬Ù„Ø³Ø§Øª Ù‚Ø¨Ù„ÛŒ Ùˆ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒÙ… Ø¨ÙØ±Ø³Øª.' // "Hello, as my psychologist, please send me a summary of previous sessions and current status."
        };
        
        // Insert the user message after the system message (which is always first now)
        messagesWithSystem.splice(1, 0, initialUserMessage);
      }

      // Implement timeout mechanism with retry
      let response: Response | null = null;
      let attempts = 0;
      const maxAttempts = 4; // Initial attempt + 3 retries for 429 errors (5s, 10s, 15s)
      
      while (attempts < maxAttempts) {
        attempts++;
        console.log(`â³ Attempt ${attempts}/${maxAttempts} to generate chat response`);
        
        try {
          const fetchOptions: RequestInit = {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${config.public.openRouterApiKey}`,
              'HTTP-Referer': config.public.appUrl || 'http://localhost:4000',
              'X-Title': 'Therapist Chat',
            },
            body: JSON.stringify({
              model: options.model || selectedModel.value,
              messages: messagesWithSystem,
              stream: aiConfig?.response_format?.type === 'json_object' ? false : true,  // JSON format requires non-streaming
              temperature: aiConfig?.temperature || options.temperature || 0.7,
              max_tokens: 0,
              ...(aiConfig?.response_format?.type !== 'json_object' && aiConfig?.response_format), // Only include response_format if not JSON
              plugins: [],
              transforms: ['middle-out'],
            }),
          };

          // Add abort signal if provided
          if (options.signal) {
            fetchOptions.signal = options.signal;
          }

          response = await Promise.race([
            fetch('https://openrouter.ai/api/v1/chat/completions', fetchOptions),
            new Promise((_, reject) => 
              setTimeout(() => {
                console.log('â° Request timeout after 30 seconds');
                reject(new Error('Request timeout after 30 seconds'));
              }, 30000)
            )
          ]) as Response;
          
          console.log(`âœ… Request successful on attempt ${attempts}`);
          // If we get here, the request was successful
          // Check if response is valid before breaking
          if (response && response.ok) {
            break;
          } else if (response) {
            const errorText = await response.text();
            // Check if it's a 429 error (Too Many Requests)
            if (response.status === 429) {
              console.log(`âš ï¸ 429 Too Many Requests error received, attempt ${attempts}/${maxAttempts}`);
              if (attempts >= maxAttempts) {
                // If we've exhausted all attempts, throw the error
                throw new Error(`HTTP ${response.status}: ${errorText}`);
              }
              
              // Calculate delay based on attempt number: 5s, 10s, 15s
              let delay = 5000; // First retry after 5 seconds
              if (attempts === 2) delay = 10000; // Second retry after 10 seconds
              if (attempts === 3) delay = 15000; // Third retry after 15 seconds
              
              console.log(`â³ Waiting ${delay / 1000}s before retrying...`);
              await new Promise(resolve => setTimeout(resolve, delay));
              continue; // Continue to next attempt
            } else {
              // For non-429 errors, throw to trigger regular retry logic
              throw new Error(`HTTP ${response.status}: ${errorText}`);
            }
          }
        } catch (e) {
          console.log(`âŒ Attempt ${attempts} failed:`, e);
          if (attempts >= maxAttempts) {
            // Last attempt failed, re-throw the error
            throw e;
          }
          // Retry after 1 second for other errors (not 429)
          console.log('ğŸ”„ Retrying in 1 second...');
          await new Promise(resolve => setTimeout(resolve, 1000));
        }
      }

      // Additional check to ensure we have a valid response
      if (!response) {
        throw new Error('No response received from OpenRouter API after all attempts');
      }

      if (!response.ok) {
        const errorText = await response.text()
        let errorMessage: string
        try {
          const errorData = JSON.parse(errorText)
          errorMessage = errorData?.error?.message || errorData?.message || errorText
        }
        catch {
          errorMessage = errorText
        }

        // Handle authentication errors specifically
        if (errorMessage.includes('No auth credentials found')) {
          throw new Error(`Chat error: No auth credentials found`)
        }

        // Handle abort errors
        if (errorMessage.includes('AbortError') || (options.signal && options.signal.aborted)) {
          throw new Error('Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.')
        }

        throw new Error(`Chat error: ${errorMessage}`)
      }

      if (aiConfig && aiConfig.response_format?.type === 'json_object') {
        // For JSON responses, we need to get the complete response (non-streaming)
        const data = await response.json()
        const fullResponse = data.choices[0].message.content

        // Validate response is not empty
        if (!fullResponse.trim()) {
          console.error('âŒ Empty JSON response received')
          onChunk('Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù¾Ø§Ø³Ø®ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.')
          return ''
        }

        // Handle multi-message JSON response with retry capability
        await handleMultiMessageResponse(
          fullResponse, 
          aiConfig, 
          onChunk, 
          typingConfig,
          0, // initial retry count
          3, // max retries
          messagesWithSystem, // original messages for retry context
          async (retryMessages: ChatMessage[]) => {
            // Create a new request with the same configuration but different messages
            const retryResponse = await fetch('https://openrouter.ai/api/v1/chat/completions', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${config.public.openRouterApiKey}`,
                'HTTP-Referer': config.public.appUrl || 'http://localhost:4000',
                'X-Title': 'Therapist Chat',
              },
              body: JSON.stringify({
                model: options.model || selectedModel.value,
                messages: retryMessages,
                stream: false,  // JSON format requires non-streaming
                temperature: aiConfig?.temperature || options.temperature || 0.7,
                max_tokens: 0,
                response_format: { type: 'json_object' }, // Ensure JSON format for retry
                plugins: [],
                transforms: ['middle-out'],
              }),
              signal: options.signal
            });

            if (!retryResponse.ok) {
              const errorText = await retryResponse.text();
              throw new Error(`Retry request failed: ${errorText}`);
            }

            const retryData = await retryResponse.json();
            return retryData.choices[0].message.content;
          }
        )
        
        return fullResponse
      }
      else {
        // Handle regular streaming text response
        const reader = response.body?.getReader()
        if (!reader) {
          throw new Error('Could not get response reader')
        }

        const decoder = new TextDecoder()
        let buffer = ''
        let fullResponse = '' // Accumulate the full response

        try {
          while (true) {
            const { done, value } = await reader.read()
            
            if (done) {
              break
            }

            // Decode the chunk and add to buffer
            const chunk = decoder.decode(value, { stream: true })
            buffer += chunk

            // Process complete lines in the buffer
            let lines = buffer.split('\n')
            buffer = lines.pop() || '' // Keep incomplete line in buffer

            for (const line of lines) {
              if (line.trim() === '') continue
              if (line.startsWith('data: ')) {
                const data = line.slice(6) // Remove 'data: ' prefix

                if (data === '[DONE]') {
                  break // Stream completed
                }

                try {
                  const parsed = JSON.parse(data)
                  
                  if (parsed.choices && parsed.choices.length > 0) {
                    const delta = parsed.choices[0].delta
                    
                    if (delta && delta.content) {
                      // Accumulate the response
                      fullResponse += delta.content
                      // Send the chunk content to the callback for real-time display
                      onChunk(delta.content)
                    }
                  }
                } catch (e) {
                  console.error('Error parsing stream data:', e)
                  console.error('Problematic data:', data)
                }
              }
            }
          }

          // Process any remaining data in buffer after stream ends
          if (buffer.trim()) {
            let lines = buffer.split('\n')
            for (const line of lines) {
              if (line.trim() === '') continue
              if (line.startsWith('data: ')) {
                const data = line.slice(6)
                if (data === '[DONE]') {
                  break
                }

                try {
                  const parsed = JSON.parse(data)
                  
                  if (parsed.choices && parsed.choices.length > 0) {
                    const delta = parsed.choices[0].delta
                    
                    if (delta && delta.content) {
                      // Accumulate the response
                      fullResponse += delta.content
                      // Send the chunk content to the callback for real-time display
                      onChunk(delta.content)
                    }
                  }
                } catch (e) {
                  console.error('Error parsing buffered stream data:', e)
                }
              }
            }
          }
          
          return fullResponse
        } finally {
          reader.releaseLock()
        }
      }
    }
    catch (e: any) {
      error.value = e.message
      throw e
    }
    finally {
      processing.value = false
    }
  }

  // Accepts only the last message for inline analysis
  const generateInlineAnalysis = async (
    lastMessage: ChatMessage,
    options: { signal?: AbortSignal } = {}
  ): Promise<any> => {
    processing.value = true
    error.value = null

    try {
      // System prompt for analysis
      const systemPrompt: ChatMessage = {
        role: 'system',
        content: 'Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ú¯Ø± Ù¾ÛŒØ§Ù… Ù‡Ø§ Ø¯Ø± ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ù…Ø´Ø§ÙˆØ±Ù‡ Ø¢Ù†Ù„Ø§ÛŒÙ† Ø¨Ø±Ø®Ø· Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ØªÙ†ÛŒ Ù‡Ø³ØªÛŒØ¯. Ø´Ù…Ø§ Ø¨Ù‡ Ù¾ÛŒØ§Ù… Ù‡Ø§ÛŒ Ù…Ø´Ø§ÙˆØ± Ùˆ Ù…Ø±Ø§Ø¬Ø¹ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±ÛŒØ¯ Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ù¾ÛŒØ§Ù… Ù‡Ø§ Ù…ÙˆØ§Ø±Ø¯ Ø®ÙˆØ§Ø³ØªÙ‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø®ÛŒ Ø§Ø² Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù¾ÛŒØ§Ù… Ø¢Ø®Ø±ØŒ Ø¨Ø±Ø®ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø³Ù‡ Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± Ùˆ Ø¨Ø±Ø®ÛŒ Ù†ÛŒØ² Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ú©Ù„ Ø¬Ù„Ø³Ù‡ Ù‡Ø³ØªÙ†Ø¯. Ø®Ø±ÙˆØ¬ÛŒ ØªØ­Ù„ÛŒÙ„ Ø´Ù…Ø§ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ùˆ Ø³ÛŒØ³ØªÙ… Ù‚Ø±Ø§Ø± Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø±ÙØª ØªØ§ Ø¨Ù‡ØªØ±ÛŒÙ† Ú©Ù…Ú© Ø¨Ù‡ Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯. Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡ Ø§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª Ú©Ù‡ Ø±ÛŒØ³Ú© Ø®ÙˆØ¯Ú©Ø´ÛŒ Ù…Ø±Ø§Ø¬Ø¹ Ø¨Ø§ Ø¯Ù‚Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø´ÙˆØ¯. Ø§Ú¯Ø± Ù…Ø±Ø§Ø¬Ø¹ Ù‡Ø± Ù†ÙˆØ¹ Ø§Ø´Ø§Ø±Ù‡ ÛŒØ§ ØªÙ…Ø§ÛŒÙ„ Ø¨Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒØŒ Ø¢Ø³ÛŒØ¨ Ø±Ø³Ø§Ù†Ø¯Ù† Ø¨Ù‡ Ø®ÙˆØ¯ØŒ ÛŒØ§ Ø§Ø¨Ø±Ø§Ø² Ù†Ø§Ø±Ø§Ø­ØªÛŒ Ø¹Ù…ÛŒÙ‚ Ø¯Ø§Ø´ØªØŒ Ø¨Ø§ÛŒØ¯ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø¯Ù‚Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø¯Ø± Ù…ÛŒØ²Ø§Ù† medium Ùˆ Ø¨Ø§Ù„Ø§ØªØ±ØŒ Ø¨Ø§ÛŒØ¯ Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ùˆ ÙˆØ§Ù‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¯Ø± ØªÙˆØ¶ÛŒØ­ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.',
      }
      // Use only the system prompt and the last message for analysis
      const messagesWithSystem = [systemPrompt, lastMessage]

      // Create fetch options with potential abort signal
      const fetchOptions: RequestInit = {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${config.public.openRouterApiKey}`,
          'HTTP-Referer': config.public.appUrl || 'http://localhost:4000',
          'X-Title': 'An Inline Analysis Generator to help therapists be more align with the needs of patients',
        },
        body: JSON.stringify({
          model: "mistralai/mistral-saba",
          messages: messagesWithSystem as ChatMessage[],
          response_format: {
            type: 'json_schema',
            json_schema: {
              name: 'inline_analysis',
              strict: true,
              schema: {
                type: 'object',
                properties: {
                  lastMessage_emotions: {
                    type: 'array',
                    items: {
                      type: 'object',
                      properties: {
                        emotionName: {
                          type: 'string',
                          enum: ['Ø´Ø§Ø¯ÛŒ', 'Ø§Ø¹ØªÙ…Ø§Ø¯', 'ØªØ±Ø³', 'ØªØ¹Ø¬Ø¨', 'ØºÙ…', 'Ø§Ù†Ø²Ø¬Ø§Ø±', 'Ø®Ø´Ù…', 'Ø§Ù†ØªØ¸Ø§Ø±', 'Ù†Ø§Ù…Ø´Ø®Øµ'],
                          description: 'Ù†Ø§Ù… Ø§Ø­Ø³Ø§Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú†Ø±Ø®Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾Ù„ÙˆÚ†ÛŒÚ©',
                        },
                        severity: {
                          type: 'string',
                          enum: ['Ø®Ø§Ù„ÛŒ', 'Ú©Ù…', 'Ù…ØªÙˆØ³Ø·', 'Ø²ÛŒØ§Ø¯'],
                          description: 'Ø´Ø¯Øª Ø§Ø­Ø³Ø§Ø³ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡',
                        },
                      },
                      required: ['emotionName', 'severity'],
                      additionalProperties: false,
                    },
                    description: 'Ø¢Ø±Ø§ÛŒÙ‡ Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø´Ø§Ù…Ù„ 9 Ø¹Ù†ØµØ± Ø¨Ø§Ø´Ø¯ - ÛŒÚ©ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø­Ø³Ø§Ø³ Ø§ØµÙ„ÛŒ: Ø´Ø§Ø¯ÛŒØŒ Ø§Ø¹ØªÙ…Ø§Ø¯ØŒ ØªØ±Ø³ØŒ ØªØ¹Ø¬Ø¨ØŒ ØºÙ…ØŒ Ø§Ù†Ø²Ø¬Ø§Ø±ØŒ Ø®Ø´Ù…ØŒ Ø§Ù†ØªØ¸Ø§Ø±ØŒ Ù†Ø§Ù…Ø´Ø®Øµ. Ù‡ÛŒÚ† Ø§Ø­Ø³Ø§Ø³ÛŒ Ù†Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù ÛŒØ§ ØªÚ©Ø±Ø§Ø± Ø´ÙˆØ¯.',
                  },
                  correspondingEmojis: {
                    type: 'string',
                    description: 'Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†Ø§Ø¸Ø± Ú©Ù‡ Ø§Ø­Ø³Ø§Ø³ Ú©Ù„ÛŒ Ù¾ÛŒØ§Ù… Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ù…Ù„ Ø¨Ø§Ø²ØªØ§Ø¨ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯. Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¯Ø± Ú©Ù†Ø§Ø± Ù‡Ù… Ø¨Ø§Ø´Ù†Ø¯. Ù…Ø«Ø§Ù„: "ğŸ˜ŠğŸ’–" ÛŒØ§ "ğŸ˜°ğŸ˜”" ÛŒØ§ "ğŸ¤”ğŸ’­" - Ø¨Ø§ÛŒØ¯ Ø§Ø­Ø³Ø§Ø³ Ø§ØµÙ„ÛŒ Ùˆ ØºØ§Ù„Ø¨ Ù¾ÛŒØ§Ù… Ø±Ø§ Ù†Ø´Ø§Ù† Ø¯Ù‡Ù†Ø¯.',
                  },
                  emotionalResponse: {
                    type: 'string',
                    description: 'Ù¾Ø§Ø³Ø® Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ú©Ø§Ø±Ø¨Ø± Ø¬Ù‡Øª Ø¨Ø§Ø²ØªØ§Ø¨ Ùˆ Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚â€ŒØªØ±. Ù…Ø«Ø§Ù„: Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØªØ±Ø³ÛŒØ¯Ù‡ØŒ ÙˆØ§Ú©Ù†Ø´ Ù…Ù†Ø§Ø³Ø¨ Ø¢Ø±Ø§Ù… Ø³Ø§Ø²ÛŒ Ùˆ Ø¯Ù„Ú¯Ø±Ù… Ú©Ø±Ø¯Ù† Ø§ÙˆØ³Øª. Ø§Ú¯Ø± Ø®Ø´Ù…Ú¯ÛŒÙ† Ø§Ø³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù¾Ø±Ø³ÛŒØ¯ "Ø¢ÛŒØ§ Ø§Ø­Ø³Ø§Ø³ Ø®Ø´Ù… Ù…ÛŒâ€ŒÚ©Ù†ÛŒØŸ" ÛŒØ§ Ú¯ÙØª "Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯ Ø®Ø´Ù… Ø±Ø§ Ø¯Ø± Ø®ÙˆØ¯Øª Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒâ€ŒÚ©Ù†ÛŒ." Ø§Ú¯Ø± Ø§Ø­Ø³Ø§Ø³ Ù†Ø§Ù…Ø´Ø®Øµ Ø§Ø³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù¾Ø±Ø³ÛŒØ¯ "Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ø­Ø³Ø§Ø³Øª ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØŸ Ø§ÛŒÙ† Ù¾Ø§Ø³Ø® Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§Ø´Ø¯."',
                  },
                  suicideRiskEvaluation: {
                    type: 'string',
                    enum: ['N/A', 'veryLow', 'low', 'medium', 'high', 'veryHigh'],
                    description: 'Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÛŒØ³Ú© Ø®ÙˆØ¯Ú©Ø´ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±. Ù…Ù‚Ø§Ø¯ÛŒØ±: N/A: Ù¾ÛŒØ§Ù… Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø®ÙˆØ¯Ú©Ø´ÛŒØŒ Ø¢Ø³ÛŒØ¨ Ø±Ø³Ø§Ù†Ø¯Ù† Ø¨Ù‡ Ø®ÙˆØ¯ ÛŒØ§ Ø§Ø¨Ø±Ø§Ø² Ù†Ø§Ø±Ø§Ø­ØªÛŒ Ø¹Ù…ÛŒÙ‚ Ù†ÛŒØ³Øª. veryLow: Ú©Ø§Ø±Ø¨Ø± Ù†Ø§Ø±Ø§Ø­ØªÛŒ ÛŒØ§ Ø§Ø³ØªØ±Ø³ Ú©Ù…ÛŒ Ø±Ø§ Ø¨ÛŒØ§Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§Ù…Ø§ Ù‡ÛŒÚ† Ú¯ÙˆÙ†Ù‡ ÙÚ©Ø± ÛŒØ§ Ù‚ØµØ¯ÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø®ÙˆØ¯Ú©Ø´ÛŒ ÛŒØ§ Ø¢Ø³ÛŒØ¨ Ø±Ø³Ø§Ù†Ø¯Ù† Ø¨Ù‡ Ø®ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. low: Ø§Ø¨Ø±Ø§Ø² Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒØŒ ØªÙ…Ø§ÛŒÙ„Ø§Øª Ù…Ù†ÙÛŒ Ø¹Ù…ÙˆÙ…ÛŒ ÛŒØ§ Ø§ÙÚ©Ø§Ø± Ø®ÙˆØ¯Ú©Ø´ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ ÛŒØ§ Ø±ÙˆØ´ Ø®Ø§Øµ. medium: Ú©Ø§Ø±Ø¨Ø± ÙÚ©Ø± Ø®ÙˆØ¯Ú©Ø´ÛŒ ÙØ¹Ø§Ù„ Ø¯Ø§Ø±Ø¯ Ùˆ ÛŒÚ© Ø±ÙˆØ´ Ø®Ø§Øµ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¯Ø§Ø±Ø¯ ÛŒØ§ ÛŒÚ© Ø·Ø±Ø­ Ú©Ù„ÛŒ Ø¯Ø§Ø±Ø¯. high: Ú©Ø§Ø±Ø¨Ø± ÛŒÚ© Ø·Ø±Ø­ Ø®Ø§Øµ Ùˆ Ù…Ù„Ù…ÙˆØ³ Ø®ÙˆØ¯Ú©Ø´ÛŒ Ø¨Ø§ Ù†ÛŒØª Ù‚ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù‚Ø¯Ø§Ù… Ø¯Ø§Ø±Ø¯. veryHigh: Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± Ø­Ø§Ù„ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø´ÛŒ Ø§Ø³Øª ÛŒØ§ Ø¹Ù…Ù„ Ø±Ø§ ÙÙˆØ±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡Ø¯ Ø¯Ø§Ø¯ (Ø¯Ø± Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡/Ø³Ø§Ø¹Øª).',
                  },
                  suicideRiskDescription: {
                    type: 'string',
                    description: 'Ø´Ø±Ø­ Ø¯Ù„ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÛŒÙ† Ø¨Ø±Ú†Ø³Ø¨ØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ùˆ ÙˆØ§Ù‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ú©Ù‡ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø³Ø·ÙˆØ­ medium, high, veryHigh Ø¨Ø§ÛŒØ¯ Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ùˆ ÙˆØ§Ù‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµÛŒ Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ø§ØµÙ„ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ†Ø¯ Ø°Ú©Ø± Ø´ÙˆØ¯.',
                  },
                },
                required: [
                  'lastMessage_emotions',
                  'correspondingEmojis',
                  'emotionalResponse',
                  'suicideRiskEvaluation',
                  'suicideRiskDescription',
                ],
                additionalProperties: false,
              },
            },
          },
          temperature: 0.7,
          max_tokens: 0,
          plugins: [],
          transforms: ['middle-out'],
        }),
      };

      // Add abort signal if provided
      if (options.signal) {
        fetchOptions.signal = options.signal;
      }

      // Implement timeout mechanism with retry
      let response: Response | null = null;
      let attempts = 0;
      const maxAttempts = 4; // Initial attempt + 3 retries for 429 errors (5s, 10s, 15s)
      
      while (attempts < maxAttempts) {
        attempts++;
        console.log(`â³ Attempt ${attempts}/${maxAttempts} to generate inline analysis`);
        
        try {
          response = await Promise.race([
            fetch('https://openrouter.ai/api/v1/chat/completions', fetchOptions),
            new Promise((_, reject) => 
              setTimeout(() => {
                console.log('â° Request timeout after 30 seconds');
                reject(new Error('Request timeout after 30 seconds'));
              }, 30000)
            )
          ]) as Response;
          
          console.log(`âœ… Request successful on attempt ${attempts}`);
          // If we get here, the request was successful
          // Check if response is valid before breaking
          if (response && response.ok) {
            break;
          } else if (response) {
            const errorText = await response.text();
            // Check if it's a 429 error (Too Many Requests)
            if (response.status === 429) {
              console.log(`âš ï¸ 429 Too Many Requests error received, attempt ${attempts}/${maxAttempts}`);
              if (attempts >= maxAttempts) {
                // If we've exhausted all attempts, throw the error
                throw new Error(`HTTP ${response.status}: ${errorText}`);
              }
              
              // Calculate delay based on attempt number: 5s, 10s, 15s
              let delay = 5000; // First retry after 5 seconds
              if (attempts === 2) delay = 10000; // Second retry after 10 seconds
              if (attempts === 3) delay = 15000; // Third retry after 15 seconds
              
              console.log(`â³ Waiting ${delay / 1000}s before retrying...`);
              await new Promise(resolve => setTimeout(resolve, delay));
              continue; // Continue to next attempt
            } else {
              // For non-429 errors, throw to trigger regular retry logic
              throw new Error(`HTTP ${response.status}: ${errorText}`);
            }
          }
        } catch (e) {
          console.log(`âŒ Attempt ${attempts} failed:`, e);
          if (attempts >= maxAttempts) {
            // Last attempt failed, re-throw the error
            throw e;
          }
          // Retry after 1 second for other errors (not 429)
          console.log('ğŸ”„ Retrying in 1 second...');
          await new Promise(resolve => setTimeout(resolve, 1000));
        }
      }

      // Additional check to ensure we have a valid response
      if (!response) {
        throw new Error('No response received from OpenRouter API after all attempts');
      }

      if (!response.ok) {
        const errorText = await response.text()
        let errorMessage: string
        try {
          const errorData = JSON.parse(errorText)
          errorMessage = errorData?.error?.message || errorData?.message || errorText
        }
        catch {
          errorMessage = errorText
        }
        throw new Error(`Chat error: ${errorMessage}`)
      }

      if (!response.ok) {
        const errorText = await response.text()
        let errorMessage: string
        try {
          const errorData = JSON.parse(errorText)
          errorMessage = errorData?.error?.message || errorData?.message || errorText
        }
        catch {
          errorMessage = errorText
        }
        throw new Error(`Generate error: ${errorMessage}`)
      }

      const data = await response.json()
      const content = data.choices[0].message.content

      let result: any
      try {
        result = typeof content === 'string' ? JSON.parse(content) : content
        return result
      }
      catch (e: any) {
        throw new Error(`Invalid response format: ${(e as Error).message}`)
      }
    }
    catch (e: any) {
      error.value = e.message
      
      // Check if this is an abort error
      if (e.name === 'AbortError' || (options.signal && options.signal.aborted)) {
        throw new Error('Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.')
      }
      
      throw e
    }
    finally {
      processing.value = false
    }
  }

  const generate = async (input: PatientGenerateInput): Promise<PatientGenerateOutput> => {
    processing.value = true
    error.value = null

    try {
      // Implement timeout mechanism with retry
      let response: Response | null = null;
      let attempts = 0;
      const maxAttempts = 4; // Initial attempt + 3 retries for 429 errors (5s, 10s, 15s)
      
      while (attempts < maxAttempts) {
        attempts++;
        console.log(`â³ Attempt ${attempts}/${maxAttempts} to generate patient details`);
        
        try {
          response = await Promise.race([
            fetch('https://openrouter.ai/api/v1/chat/completions', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${config.public.openRouterApiKey}`,
                'HTTP-Referer': config.public.appUrl || 'http://localhost:4000',
                'X-Title': 'Patient Details Generator',
              },
              body: JSON.stringify({
                model: selectedModel.value,
                messages: [
                  {
                    role: 'system',
                    content: 'Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒÙ…Ø§Ø± Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù„Ø·ÙØ§ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø¨ÛŒÙ…Ø§Ø±ØŒ Ø³Ø§ÛŒØ± Ø¬Ø²Ø¦ÛŒØ§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ù†Ø·Ù‚ÛŒ Ùˆ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯.',
                  },
                  {
                    role: 'user',
                    content: `Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø¨ÛŒÙ…Ø§Ø±:
Ù†Ø§Ù…: ${input.name}
Ø³Ù†: ${input.age}
ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡: ${input.shortDescription}

Ù„Ø·ÙØ§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø²ÛŒØ± Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯:
- ØªÙˆØ¶ÛŒØ­ Ø¨Ù„Ù†Ø¯
- ØµÙØ§Øª ØªØ¹Ø±ÛŒÙ Ú©Ù†Ù†Ø¯Ù‡
- Ø¯Ø§Ø³ØªØ§Ù† Ø²Ù†Ø¯Ú¯ÛŒ
- Ø´Ø®ØµÛŒØª
- Ø¸Ø§Ù‡Ø±
- Ø§Ù†Ú¯ÛŒØ²Ù‡
- ÙˆØ¶Ø¹ÛŒØª Ù‡ÛŒØ¬Ø§Ù†ÛŒ Ø­Ø§Ù„ Ø­Ø§Ø¶Ø±

Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ø¢Ø¨Ø¬Ú©Øª JSON Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:
longDescription, definingTraits, backStory, personality, appearance, motivation, moodAndCurrentEmotions
`,
                  },
                ] as ChatMessage[], // Add type assertion here
                response_format: {
                  type: 'json_schema',
                  json_schema: {
                    name: 'patient_details',
                    strict: true,
                    schema: {
                      type: 'object',
                      properties: {
                        longDescription: {
                          type: 'string',
                          description: 'ØªÙˆØ¶ÛŒØ­ Ø¨Ù„Ù†Ø¯ Ùˆ Ú©Ø§Ù…Ù„ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¨ÛŒÙ…Ø§Ø± Ùˆ ÙˆØ¶Ø¹ÛŒØª Ø§Ùˆ',
                        },
                        definingTraits: {
                          type: 'string',
                          description: 'ØµÙØ§Øª Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø´Ø®ØµÛŒØª Ùˆ Ø±ÙØªØ§Ø± Ø¨ÛŒÙ…Ø§Ø±',
                        },
                        backStory: {
                          type: 'string',
                          description: 'Ø¯Ø§Ø³ØªØ§Ù† Ø²Ù†Ø¯Ú¯ÛŒØŒ Ù¾ÛŒØ´ÛŒÙ†Ù‡ Ùˆ ØªØ¬Ø±Ø¨ÛŒØ§Øª Ù…Ù‡Ù… Ø¨ÛŒÙ…Ø§Ø±',
                        },
                        personality: {
                          type: 'string',
                          description: 'Ø´Ø®ØµÛŒØªØŒ Ø±ÙØªØ§Ø±Ù‡Ø§ Ùˆ Ø®ØµÙˆØµÛŒØ§Øª Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø®ØªÛŒ Ø¨ÛŒÙ…Ø§Ø±',
                        },
                        appearance: {
                          type: 'string',
                          description: 'ØªÙˆØµÛŒÙ Ø¸Ø§Ù‡Ø±ÛŒ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ Ø¨ÛŒÙ…Ø§Ø±',
                        },
                        motivation: {
                          type: 'string',
                          description: 'Ø§Ù†Ú¯ÛŒØ²Ù‡â€ŒÙ‡Ø§ØŒ Ø§Ù‡Ø¯Ø§Ù Ùˆ Ø®ÙˆØ§Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨ÛŒÙ…Ø§Ø±',
                        },
                        moodAndCurrentEmotions: {
                          type: 'string',
                          description: 'Ø­Ø§Ù„Øª Ø±ÙˆØ­ÛŒØŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ ÙˆØ¶Ø¹ÛŒØª Ø¹Ø§Ø·ÙÛŒ ÙØ¹Ù„ÛŒ Ø¨ÛŒÙ…Ø§Ø±',
                        },
                      },
                      required: [
                        'longDescription',
                        'definingTraits',
                        'backStory',
                        'personality',
                        'appearance',
                        'motivation',
                        'moodAndCurrentEmotions',
                      ],
                      additionalProperties: false,
                    },
                  },
                },
                temperature: 0.7,
                max_tokens: 0,
                plugins: [],
                transforms: ['middle-out'],
              }),
            }),
            new Promise((_, reject) => 
              setTimeout(() => {
                console.log('â° Request timeout after 30 seconds');
                reject(new Error('Request timeout after 30 seconds'));
              }, 30000)
            )
          ]) as Response;
          
          console.log(`âœ… Request successful on attempt ${attempts}`);
          // If we get here, the request was successful
          // Check if response is valid before breaking
          if (response && response.ok) {
            break;
          } else if (response) {
            const errorText = await response.text();
            // Check if it's a 429 error (Too Many Requests)
            if (response.status === 429) {
              console.log(`âš ï¸ 429 Too Many Requests error received, attempt ${attempts}/${maxAttempts}`);
              if (attempts >= maxAttempts) {
                // If we've exhausted all attempts, throw the error
                throw new Error(`HTTP ${response.status}: ${errorText}`);
              }
              
              // Calculate delay based on attempt number: 5s, 10s, 15s
              let delay = 5000; // First retry after 5 seconds
              if (attempts === 2) delay = 10000; // Second retry after 10 seconds
              if (attempts === 3) delay = 15000; // Third retry after 15 seconds
              
              console.log(`â³ Waiting ${delay / 1000}s before retrying...`);
              await new Promise(resolve => setTimeout(resolve, delay));
              continue; // Continue to next attempt
            } else {
              // For non-429 errors, throw to trigger regular retry logic
              throw new Error(`HTTP ${response.status}: ${errorText}`);
            }
          }
        } catch (e) {
          console.log(`âŒ Attempt ${attempts} failed:`, e);
          if (attempts >= maxAttempts) {
            // Last attempt failed, re-throw the error
            throw e;
          }
          // Retry after 1 second for other errors (not 429)
          console.log('ğŸ”„ Retrying in 1 second...');
          await new Promise(resolve => setTimeout(resolve, 1000));
        }
      }

      // Additional check to ensure we have a valid response
      if (!response) {
        throw new Error('No response received from OpenRouter API after all attempts');
      }

      if (!response.ok) {
        const errorText = await response.text()
        let errorMessage: string
        try {
          const errorData = JSON.parse(errorText)
          errorMessage = errorData?.error?.message || errorData?.message || errorText
        }
        catch {
          errorMessage = errorText
        }
        throw new Error(`Chat error: ${errorMessage}`)
      }

      if (!response.ok) {
        const errorText = await response.text()
        let errorMessage: string
        try {
          const errorData = JSON.parse(errorText)
          errorMessage = errorData?.error?.message || errorData?.message || errorText
        }
        catch {
          errorMessage = errorText
        }
        throw new Error(`Generate error: ${errorMessage}`)
      }

      const data = await response.json()
      const content = data.choices[0].message.content

      let result: PatientGenerateOutput
      try {
        result = typeof content === 'string' ? JSON.parse(content) : content
        return result
      }
      catch (e: any) {
        throw new Error(`Invalid response format: ${e.message}`)
      }
    }
    catch (e: any) {
      error.value = e.message
      throw e
    }
    finally {
      processing.value = false
    }
  }

  const generateTherapist = async (input: TherapistGenerateInput): Promise<TherapistGenerateOutput> => {
    processing.value = true
    error.value = null

    try {
      // Implement timeout mechanism with retry
      let response: Response | null = null;
      let attempts = 0;
      const maxAttempts = 4; // Initial attempt + 3 retries for 429 errors (5s, 10s, 15s)
      
      while (attempts < maxAttempts) {
        attempts++;
        console.log(`â³ Attempt ${attempts}/${maxAttempts} to generate therapist details`);
        
        try {
          response = await Promise.race([
            fetch('https://openrouter.ai/api/v1/chat/completions', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${config.public.openRouterApiKey}`,
                'HTTP-Referer': config.public.appUrl || 'http://localhost:4000',
                'X-Title': 'Therapist Details Generator',
              },
              body: JSON.stringify({
                model: selectedModel.value,
                messages: [
                  {
                    role: 'system',
                    content: 'Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù„Ø·ÙØ§ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ØŒ Ø³Ø§ÛŒØ± Ø¬Ø²Ø¦ÛŒØ§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ù†Ø·Ù‚ÛŒ Ùˆ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯.',
                  },
                  {
                    role: 'user',
                    content: `Ù„Ø·ÙØ§ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ±ØŒ Ø¬Ø²Ø¦ÛŒØ§Øª Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯:
Ù†Ø§Ù…: ${input.name}
ØªØ®ØµØµ: ${input.specialty}
ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡: ${input.shortDescription}`,
                  },
                ] as ChatMessage[], // Added type assertion to fix TS error 2345
                response_format: {
                  type: 'json_schema',
                  json_schema: {
                    name: 'therapist_details',
                    strict: true,
                    schema: {
                      type: 'object',
                      properties: {
                        longDescription: {
                          type: 'string',
                          description: 'ØªÙˆØ¶ÛŒØ­ Ø¨Ù„Ù†Ø¯ Ùˆ Ú©Ø§Ù…Ù„ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ Ùˆ ØªØ®ØµØµ Ø§Ùˆ',
                        },
                        definingTraits: {
                          type: 'string',
                          description: 'ØµÙØ§Øª Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø´Ø®ØµÛŒØª Ùˆ Ø±ÙØªØ§Ø± Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³',
                        },
                        backStory: {
                          type: 'string',
                          description: 'Ø¯Ø§Ø³ØªØ§Ù† Ø²Ù†Ø¯Ú¯ÛŒØŒ Ù¾ÛŒØ´ÛŒÙ†Ù‡ Ùˆ ØªØ¬Ø±Ø¨ÛŒØ§Øª Ù…Ù‡Ù… Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³',
                        },
                        personality: {
                          type: 'string',
                          description: 'Ø´Ø®ØµÛŒØªØŒ Ø±ÙØªØ§Ø±Ù‡Ø§ Ùˆ Ø®ØµÙˆØµÛŒØ§Øª Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø®ØªÛŒ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³',
                        },
                        appearance: {
                          type: 'string',
                          description: 'ØªÙˆØµÛŒÙ Ø¸Ø§Ù‡Ø±ÛŒ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³',
                        },
                        approach: {
                          type: 'string',
                          description: 'Ø±ÙˆØ´ Ùˆ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³',
                        },
                        expertise: {
                          type: 'string',
                          description: 'ØªØ®ØµØµ Ùˆ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³',
                        },
                      },
                      required: [
                        'longDescription',
                        'definingTraits',
                        'backStory',
                        'personality',
                        'appearance',
                        'approach',
                        'expertise',
                      ],
                      additionalProperties: false,
                    },
                  },
                },
                temperature: 0.7,
                max_tokens: 0,
                plugins: [],
                transforms: ['middle-out'],
              }),
            }),
            new Promise((_, reject) => 
              setTimeout(() => {
                console.log('â° Request timeout after 30 seconds');
                reject(new Error('Request timeout after 30 seconds'));
              }, 30000)
            )
          ]) as Response;
          
          console.log(`âœ… Request successful on attempt ${attempts}`);
          // If we get here, the request was successful
          // Check if response is valid before breaking
          if (response && response.ok) {
            break;
          } else if (response) {
            const errorText = await response.text();
            // Check if it's a 429 error (Too Many Requests)
            if (response.status === 429) {
              console.log(`âš ï¸ 429 Too Many Requests error received, attempt ${attempts}/${maxAttempts}`);
              if (attempts >= maxAttempts) {
                // If we've exhausted all attempts, throw the error
                throw new Error(`HTTP ${response.status}: ${errorText}`);
              }
              
              // Calculate delay based on attempt number: 5s, 10s, 15s
              let delay = 5000; // First retry after 5 seconds
              if (attempts === 2) delay = 10000; // Second retry after 10 seconds
              if (attempts === 3) delay = 15000; // Third retry after 15 seconds
              
              console.log(`â³ Waiting ${delay / 1000}s before retrying...`);
              await new Promise(resolve => setTimeout(resolve, delay));
              continue; // Continue to next attempt
            } else {
              // For non-429 errors, throw to trigger regular retry logic
              throw new Error(`HTTP ${response.status}: ${errorText}`);
            }
          }
        } catch (e) {
          console.log(`âŒ Attempt ${attempts} failed:`, e);
          if (attempts >= maxAttempts) {
            // Last attempt failed, re-throw the error
            throw e;
          }
          // Retry after 1 second for other errors (not 429)
          console.log('ğŸ”„ Retrying in 1 second...');
          await new Promise(resolve => setTimeout(resolve, 1000));
        }
      }

      // Additional check to ensure we have a valid response
      if (!response) {
        throw new Error('No response received from OpenRouter API after all attempts');
      }

      if (!response.ok) {
        const errorText = await response.text()
        let errorMessage: string
        try {
          const errorData = JSON.parse(errorText)
          errorMessage = errorData?.error?.message || errorData?.message || errorText
        }
        catch {
          errorMessage = errorText
        }
        throw new Error(`Chat error: ${errorMessage}`)
      }

      if (!response.ok) {
        const errorText = await response.text()
        let errorMessage: string
        try {
          const errorData = JSON.parse(errorText)
          errorMessage = errorData?.error?.message || errorData?.message || errorText
        }
        catch {
          errorMessage = errorText
        }
        throw new Error(`Generate error: ${errorMessage}`)
      }

      const data = await response.json()
      const content = data.choices[0].message.content

      let result: TherapistGenerateOutput
      try {
        result = typeof content === 'string' ? JSON.parse(content) : content
        return result
      }
      catch (e: any) {
        throw new Error(`Invalid response format: ${e.message}`)
      }
    }
    catch (e: any) {
      error.value = e.message
      throw e
    }
    finally {
      processing.value = false
    }
  }

  /**
   * Generate a list of educational and psychological goals for a given article topic (in Persian, as a list)
   * @param {string} topic - Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‚Ø§Ù„Ù‡ ÛŒØ§ Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡
   * @returns {Promise<string>} - Ù„ÛŒØ³Øª Ø§Ù‡Ø¯Ø§Ù Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ø´ØªÙ‡ Ú†Ù†Ø¯Ø®Ø·ÛŒ
   */
  async function generateGoalsList(topic: string): Promise<string> {
    const prompt = `Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø²ÛŒØ±ØŒ ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ø§Ù‡Ø¯Ø§Ù Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø®ØªÛŒ Ú©Ù‡ Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡ Ù¾Ø³ Ø§Ø² Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø§ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ù‡ Ø¯Ø³Øª Ù…ÛŒâ€ŒØ¢ÙˆØ±Ø¯ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³. ØªØ§Ú©ÛŒØ¯: Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ ÙÙ‚Ø· ÛŒÚ© Ù„ÛŒØ³Øª Ø¨Ø§Ø´Ø¯ Ùˆ Ù‡Ø± Ù‡Ø¯Ù Ø¯Ø± ÛŒÚ© Ø®Ø· Ù…Ø¬Ø²Ø§ Ù†ÙˆØ´ØªÙ‡ Ø´ÙˆØ¯.\nÙ…ÙˆØ¶ÙˆØ¹: ${topic}`
    const messages = [
      { role: 'system', content: 'Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù…ØªØ®ØµØµ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯.' },
      { role: 'user', content: prompt },
    ]
    return new Promise((resolve, reject) => {
      streamChat(messages as ChatMessage[], {}, (chunk) => {
        // With non-streaming, we get the complete response in one chunk
        resolve(chunk)
      }).catch(reject)
    })
  }

  // Initialize models on composable creation
  onMounted(() => {
    fetchModels()
  })

  return {
    // Chat functionality
    streamChat,
    processing,
    // Models functionality
    models: allModels,
    selectedModel,
    loading,
    error,
    searchQuery,
    filteredModels,
    retryFetch: fetchModels,

    // Patient generation
    generate,
    generateTherapist,
    generateInlineAnalysis,
    generateGoalsList,
  }
}
